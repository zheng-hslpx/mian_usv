#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
USVå¯¹æ¯”å®éªŒç»Ÿä¸€æµ‹è¯•æ¡†æ¶
æ”¯æŒæ‰€æœ‰ç®—æ³•çš„ç»Ÿä¸€æµ‹è¯•ã€å¯¹æ¯”åˆ†æå’Œç»“æœç”Ÿæˆ

ç®—æ³•æ”¯æŒï¼š
- è°ƒåº¦è§„åˆ™ç®—æ³•ï¼ˆ5ä¸ªï¼‰ï¼šæœ€è¿‘ä»»åŠ¡ä¼˜å…ˆã€æœ€ä½ç”µé‡ä¼˜å…ˆã€æœ€è¿œä»»åŠ¡ä¼˜å…ˆã€æœ€é«˜ç”µé‡ä¼˜å…ˆã€éšæœºè§„åˆ’å™¨
- å…ƒå¯å‘å¼ç®—æ³•ï¼ˆ3ä¸ªï¼‰ï¼šäººå·¥èœ‚ç¾¤ç®—æ³•ã€é—ä¼ ç®—æ³•ã€ç²’å­ç¾¤ç®—æ³•
- å­¦ä¹ ç®—æ³•ï¼ˆ2ä¸ªï¼‰ï¼šDQNç®—æ³•ã€PPOç®—æ³•

åŠŸèƒ½ç‰¹æ€§ï¼š
- ç»Ÿä¸€æ¥å£è°ƒç”¨æ‰€æœ‰ç®—æ³•
- è‡ªåŠ¨é€‰æ‹©ä»£è¡¨æ€§æµ‹è¯•æ¡ˆä¾‹
- ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”åˆ†ææŠ¥å‘Š
- æ”¯æŒç»“æœå¯è§†åŒ–å’Œå¯¼å‡º
"""

import os
import sys
import json
import time
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Tuple
import traceback

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'contrast_experiment'))

# å¯¼å…¥è°ƒåº¦è§„åˆ™ç®—æ³•
from contrast_experiment.dispatching_rule_methods.task_nearest_distant_first_modular import run_single_case as run_nearest_first
from contrast_experiment.dispatching_rule_methods.usv_lowest_battery_first_modular import run_single_case as run_lowest_battery
from contrast_experiment.dispatching_rule_methods.task_farthest_distant_first_modular import run_single_case as run_farthest_first
from contrast_experiment.dispatching_rule_methods.usv_highest_battery_first_modular import run_single_case as run_highest_battery
from contrast_experiment.dispatching_rule_methods.usv_task_random_planner_modular import run_single_case as run_random_planner

# å¯¼å…¥å…ƒå¯å‘å¼ç®—æ³•
from contrast_experiment.meta_heuristic_methods.abc_task_planner_modular import run_single_case as run_abc
from contrast_experiment.meta_heuristic_methods.ga_task_planner_modular import run_single_case as run_ga
from contrast_experiment.meta_heuristic_methods.pso_task_planner_modular import run_single_case as run_pso

# å¯¼å…¥å­¦ä¹ ç®—æ³•
from contrast_experiment.learning_based_methods.dqn_task_planner_modular import run_single_case as run_dqn
from contrast_experiment.learning_based_methods.ppo_task_planner_modular import run_single_case as run_ppo


class USVTestFramework:
    """USVç»Ÿä¸€æµ‹è¯•æ¡†æ¶"""

    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶"""
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(self.base_dir, 'usv_data_dev')
        self.save_dir = os.path.join(self.base_dir, 'save')
        self.results_dir = os.path.join(self.base_dir, 'test_results')

        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs(self.results_dir, exist_ok=True)

        # ç®—æ³•é…ç½®
        self.algorithms = {
            # è°ƒåº¦è§„åˆ™ç®—æ³•
            'æœ€è¿‘ä»»åŠ¡ä¼˜å…ˆ': {
                'func': run_nearest_first,
                'type': 'dispatch_rule',
                'description': 'ä¼˜å…ˆåˆ†é…è·ç¦»USVæœ€è¿‘çš„ä»»åŠ¡'
            },
            'æœ€ä½ç”µé‡ä¼˜å…ˆ': {
                'func': run_lowest_battery,
                'type': 'dispatch_rule',
                'description': 'ä¼˜å…ˆåˆ†é…ç»™ç”µé‡æœ€ä½çš„USV'
            },
            'æœ€è¿œä»»åŠ¡ä¼˜å…ˆ': {
                'func': run_farthest_first,
                'type': 'dispatch_rule',
                'description': 'ä¼˜å…ˆåˆ†é…è·ç¦»USVæœ€è¿œçš„ä»»åŠ¡'
            },
            'æœ€é«˜ç”µé‡ä¼˜å…ˆ': {
                'func': run_highest_battery,
                'type': 'dispatch_rule',
                'description': 'ä¼˜å…ˆåˆ†é…ç»™ç”µé‡æœ€é«˜çš„USV'
            },
            'éšæœºè§„åˆ’å™¨': {
                'func': run_random_planner,
                'type': 'dispatch_rule',
                'description': 'éšæœºåˆ†é…ä»»åŠ¡ç»™USV'
            },

            # å…ƒå¯å‘å¼ç®—æ³•
            'äººå·¥èœ‚ç¾¤ç®—æ³•': {
                'func': run_abc,
                'type': 'meta_heuristic',
                'description': 'ä½¿ç”¨äººå·¥èœ‚ç¾¤ç®—æ³•ä¼˜åŒ–ä»»åŠ¡åˆ†é…'
            },
            'é—ä¼ ç®—æ³•': {
                'func': run_ga,
                'type': 'meta_heuristic',
                'description': 'ä½¿ç”¨é—ä¼ ç®—æ³•ä¼˜åŒ–ä»»åŠ¡åˆ†é…'
            },
            'ç²’å­ç¾¤ç®—æ³•': {
                'func': run_pso,
                'type': 'meta_heuristic',
                'description': 'ä½¿ç”¨ç²’å­ç¾¤ç®—æ³•ä¼˜åŒ–ä»»åŠ¡åˆ†é…'
            },

            # å­¦ä¹ ç®—æ³•
            'DQNç®—æ³•': {
                'func': run_dqn,
                'type': 'learning_based',
                'description': 'ä½¿ç”¨æ·±åº¦Qå­¦ä¹ ä¼˜åŒ–ä»»åŠ¡åˆ†é…'
            },
            'PPOç®—æ³•': {
                'func': run_ppo,
                'type': 'learning_based',
                'description': 'ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ä¼˜åŒ–ä»»åŠ¡åˆ†é…'
            }
        }

        # æµ‹è¯•ç»“æœ
        self.test_results = []

    def select_test_cases(self, max_cases: int = 20) -> List[str]:
        """é€‰æ‹©ä»£è¡¨æ€§æµ‹è¯•æ¡ˆä¾‹"""
        print("æ­£åœ¨é€‰æ‹©ä»£è¡¨æ€§æµ‹è¯•æ¡ˆä¾‹...")

        test_cases = []

        # éå†æ‰€æœ‰ç›®å½•ç»“æ„
        for root, dirs, files in os.walk(self.data_dir):
            for file in files:
                if file.endswith('.json') and 'instance_01' in file:
                    full_path = os.path.join(root, file)

                    # è§£ææ–‡ä»¶åè·å–ä»»åŠ¡å’ŒUSVæ•°é‡
                    dir_name = os.path.basename(root)
                    if '_' in dir_name:
                        try:
                            tasks, usvs = dir_name.split('_')
                            tasks, usvs = int(tasks), int(usvs)

                            # é€‰æ‹©ä¸åŒè§„æ¨¡çš„æ¡ˆä¾‹
                            if (tasks, usvs) in [(40, 2), (40, 4), (40, 6), (40, 8),
                                               (60, 2), (60, 4), (60, 6), (60, 8),
                                               (80, 2), (80, 4), (80, 6), (80, 8),
                                               (100, 2), (100, 4), (100, 6), (100, 8),
                                               (120, 2), (120, 4), (120, 6), (120, 8)]:
                                test_cases.append(full_path)

                                if len(test_cases) >= max_cases:
                                    break
                        except ValueError:
                            continue

            if len(test_cases) >= max_cases:
                break

        print(f"å·²é€‰æ‹© {len(test_cases)} ä¸ªä»£è¡¨æ€§æµ‹è¯•æ¡ˆä¾‹")
        return test_cases[:max_cases]

    def run_algorithm(self, algorithm_name: str, test_case: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªç®—æ³•"""
        print(f"  æ­£åœ¨è¿è¡Œ {algorithm_name}...")

        try:
            start_time = time.time()

            # è°ƒç”¨ç®—æ³•
            algorithm_info = self.algorithms[algorithm_name]
            result = algorithm_info['func'](test_case)

            end_time = time.time()
            execution_time = end_time - start_time

            # æå–ç»“æœä¿¡æ¯
            if result.get('success', False):
                makespan = result.get('makespan', float('inf'))
                metrics = result.get('metrics', {})
                assigned_tasks = metrics.get('assigned_tasks', 0)
                unassigned_tasks = metrics.get('unassigned_tasks', 0)

                # è®¡ç®—ä»»åŠ¡å®Œæˆç‡
                total_tasks = assigned_tasks + unassigned_tasks
                completion_rate = assigned_tasks / total_tasks if total_tasks > 0 else 0

                return {
                    'success': True,
                    'makespan': makespan,
                    'assigned_tasks': assigned_tasks,
                    'unassigned_tasks': unassigned_tasks,
                    'completion_rate': completion_rate,
                    'execution_time': execution_time,
                    'metrics': metrics
                }
            else:
                return {
                    'success': False,
                    'error': result.get('error', 'æœªçŸ¥é”™è¯¯'),
                    'execution_time': execution_time
                }

        except Exception as e:
            print(f"    é”™è¯¯: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'execution_time': 0,
                'traceback': traceback.format_exc()
            }

    def run_comprehensive_test(self, test_cases: List[str] = None,
                             selected_algorithms: List[str] = None) -> pd.DataFrame:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("å¼€å§‹è¿è¡Œç»¼åˆæµ‹è¯•...")
        print("=" * 60)

        if test_cases is None:
            test_cases = self.select_test_cases()

        if selected_algorithms is None:
            selected_algorithms = list(self.algorithms.keys())

        # å‡†å¤‡ç»“æœå­˜å‚¨
        results = []

        for i, test_case in enumerate(test_cases):
            print(f"\næµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(test_cases)}: {os.path.basename(test_case)}")
            print("-" * 50)

            # è§£ææµ‹è¯•æ¡ˆä¾‹ä¿¡æ¯
            dir_name = os.path.basename(os.path.dirname(test_case))
            tasks, usvs = dir_name.split('_')
            tasks, usvs = int(tasks), int(usvs)

            case_result = {
                'test_case': os.path.basename(test_case),
                'tasks': tasks,
                'usvs': usvs
            }

            # è¿è¡Œæ¯ä¸ªç®—æ³•
            for algorithm_name in selected_algorithms:
                result = self.run_algorithm(algorithm_name, test_case)

                # å­˜å‚¨ç»“æœ
                case_result[f'{algorithm_name}_success'] = result['success']
                case_result[f'{algorithm_name}_makespan'] = result.get('makespan', float('inf'))
                case_result[f'{algorithm_name}_completion_rate'] = result.get('completion_rate', 0)
                case_result[f'{algorithm_name}_execution_time'] = result.get('execution_time', 0)
                case_result[f'{algorithm_name}_assigned'] = result.get('assigned_tasks', 0)
                case_result[f'{algorithm_name}_unassigned'] = result.get('unassigned_tasks', 0)

                if not result['success']:
                    case_result[f'{algorithm_name}_error'] = result.get('error', '')

            results.append(case_result)

        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(results)
        self.test_results = df

        print(f"\nç»¼åˆæµ‹è¯•å®Œæˆï¼å…±æµ‹è¯• {len(test_cases)} ä¸ªæ¡ˆä¾‹ï¼Œ{len(selected_algorithms)} ä¸ªç®—æ³•")
        return df

    def generate_analysis_report(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\næ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        report = {
            'summary': {},
            'algorithm_comparison': {},
            'detailed_results': {}
        }

        # ç®—æ³•åˆ—è¡¨
        algorithms = [alg for alg in self.algorithms.keys() if f'{alg}_success' in df.columns]

        # åŸºæœ¬ç»Ÿè®¡
        report['summary'] = {
            'total_test_cases': len(df),
            'total_algorithms': len(algorithms),
            'test_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # ç®—æ³•å¯¹æ¯”ç»Ÿè®¡
        for alg in algorithms:
            success_col = f'{alg}_success'
            makespan_col = f'{alg}_makespan'
            completion_col = f'{alg}_completion_rate'

            # æˆåŠŸç‡
            success_rate = df[success_col].mean()

            # å¹³å‡å®Œæˆæ—¶é—´ï¼ˆåªè€ƒè™‘æˆåŠŸçš„æ¡ˆä¾‹ï¼‰
            successful_cases = df[df[success_col] == True]
            avg_makespan = successful_cases[makespan_col].mean() if len(successful_cases) > 0 else float('inf')

            # å¹³å‡å®Œæˆç‡
            avg_completion = df[completion_col].mean()

            # å¹³å‡æ‰§è¡Œæ—¶é—´
            exec_time_col = f'{alg}_execution_time'
            avg_exec_time = df[exec_time_col].mean()

            report['algorithm_comparison'][alg] = {
                'success_rate': success_rate,
                'avg_makespan': avg_makespan if avg_makespan != float('inf') else None,
                'avg_completion_rate': avg_completion,
                'avg_execution_time': avg_exec_time,
                'type': self.algorithms[alg]['type'],
                'description': self.algorithms[alg]['description']
            }

        # æ’ååˆ†æ
        rankings = {}

        # å®Œæˆæ—¶é—´æ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        makespan_ranking = []
        for alg in algorithms:
            avg_makespan = report['algorithm_comparison'][alg]['avg_makespan']
            if avg_makespan is not None:
                makespan_ranking.append((alg, avg_makespan))

        makespan_ranking.sort(key=lambda x: x[1])
        rankings['makespan_ranking'] = makespan_ranking

        # å®Œæˆç‡æ’åï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        completion_ranking = []
        for alg in algorithms:
            completion_rate = report['algorithm_comparison'][alg]['avg_completion_rate']
            completion_ranking.append((alg, completion_rate))

        completion_ranking.sort(key=lambda x: x[1], reverse=True)
        rankings['completion_ranking'] = completion_ranking

        # æˆåŠŸç‡æ’åï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        success_ranking = []
        for alg in algorithms:
            success_rate = report['algorithm_comparison'][alg]['success_rate']
            success_ranking.append((alg, success_rate))

        success_ranking.sort(key=lambda x: x[1], reverse=True)
        rankings['success_ranking'] = success_ranking

        report['rankings'] = rankings

        return report

    def save_results(self, df: pd.DataFrame, report: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        print("æ­£åœ¨ä¿å­˜æµ‹è¯•ç»“æœ...")

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = os.path.join(self.results_dir, f'test_results_{timestamp}.csv')
        df.to_csv(results_file, index=False, encoding='utf-8-sig')

        # ä¿å­˜åˆ†ææŠ¥å‘Š
        report_file = os.path.join(self.results_dir, f'analysis_report_{timestamp}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        comprehensive_file = os.path.join(self.results_dir, f'comprehensive_report_{timestamp}.txt')
        self._generate_text_report(df, report, comprehensive_file)

        print(f"ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  è¯¦ç»†ç»“æœ: {results_file}")
        print(f"  åˆ†ææŠ¥å‘Š: {report_file}")
        print(f"  ç»¼åˆæŠ¥å‘Š: {comprehensive_file}")

    def _generate_text_report(self, df: pd.DataFrame, report: Dict[str, Any], output_file: str):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„ç»¼åˆæŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("USVä»»åŠ¡è°ƒåº¦ç®—æ³•å¯¹æ¯”å®éªŒç»¼åˆæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # åŸºæœ¬ä¿¡æ¯
            summary = report['summary']
            f.write(f"æµ‹è¯•æ—¶é—´: {summary['test_date']}\n")
            f.write(f"æµ‹è¯•æ¡ˆä¾‹æ•°é‡: {summary['total_test_cases']}\n")
            f.write(f"ç®—æ³•æ•°é‡: {summary['total_algorithms']}\n\n")

            # ç®—æ³•ç±»å‹ç»Ÿè®¡
            f.write("ç®—æ³•ç±»å‹åˆ†å¸ƒ:\n")
            type_count = {}
            for alg, info in self.algorithms.items():
                alg_type = info['type']
                type_count[alg_type] = type_count.get(alg_type, 0) + 1

            for alg_type, count in type_count.items():
                f.write(f"  {alg_type}: {count}ä¸ª\n")
            f.write("\n")

            # æ’åç»“æœ
            rankings = report['rankings']

            f.write("ç®—æ³•æ€§èƒ½æ’å:\n\n")

            f.write("1. å®Œæˆæ—¶é—´æ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰:\n")
            for i, (alg, avg_time) in enumerate(rankings['makespan_ranking'][:5], 1):
                f.write(f"   {i}. {alg}: {avg_time:.2f}\n")
            f.write("\n")

            f.write("2. ä»»åŠ¡å®Œæˆç‡æ’åï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰:\n")
            for i, (alg, rate) in enumerate(rankings['completion_ranking'][:5], 1):
                f.write(f"   {i}. {alg}: {rate:.2%}\n")
            f.write("\n")

            f.write("3. æˆåŠŸç‡æ’åï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰:\n")
            for i, (alg, rate) in enumerate(rankings['success_ranking'][:5], 1):
                f.write(f"   {i}. {alg}: {rate:.2%}\n")
            f.write("\n")

            # è¯¦ç»†ç®—æ³•ä¿¡æ¯
            f.write("ç®—æ³•è¯¦ç»†ä¿¡æ¯:\n")
            f.write("-" * 30 + "\n")

            for alg, comparison in report['algorithm_comparison'].items():
                f.write(f"\n{alg}:\n")
                f.write(f"  ç±»å‹: {comparison['type']}\n")
                f.write(f"  æè¿°: {comparison['description']}\n")
                f.write(f"  æˆåŠŸç‡: {comparison['success_rate']:.2%}\n")

                if comparison['avg_makespan'] is not None:
                    f.write(f"  å¹³å‡å®Œæˆæ—¶é—´: {comparison['avg_makespan']:.2f}\n")
                else:
                    f.write(f"  å¹³å‡å®Œæˆæ—¶é—´: æ— æˆåŠŸæ¡ˆä¾‹\n")

                f.write(f"  å¹³å‡ä»»åŠ¡å®Œæˆç‡: {comparison['avg_completion_rate']:.2%}\n")
                f.write(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {comparison['avg_execution_time']:.2f}ç§’\n")

    def run_full_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
        print("USVä»»åŠ¡è°ƒåº¦ç®—æ³•å¯¹æ¯”å®éªŒ")
        print("=" * 60)

        # é€‰æ‹©æµ‹è¯•æ¡ˆä¾‹
        test_cases = self.select_test_cases(20)

        # è¿è¡Œæµ‹è¯•
        df = self.run_comprehensive_test(test_cases)

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = self.generate_analysis_report(df)

        # ä¿å­˜ç»“æœ
        self.save_results(df, report)

        # æ˜¾ç¤ºç®€è¦ç»“æœ
        self._display_summary(report)

        return df, report

    def _display_summary(self, report: Dict[str, Any]):
        """æ˜¾ç¤ºç®€è¦ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 60)

        rankings = report['rankings']

        print("\nğŸ† æ€§èƒ½æœ€ä½³ç®—æ³•:")

        print("\n1. å®Œæˆæ—¶é—´æœ€å¿«ï¼ˆå‰3åï¼‰:")
        for i, (alg, time) in enumerate(rankings['makespan_ranking'][:3], 1):
            print(f"   {i}. {alg}: {time:.2f}")

        print("\n2. ä»»åŠ¡å®Œæˆç‡æœ€é«˜ï¼ˆå‰3åï¼‰:")
        for i, (alg, rate) in enumerate(rankings['completion_ranking'][:3], 1):
            print(f"   {i}. {alg}: {rate:.2%}")

        print("\n3. æˆåŠŸç‡æœ€é«˜ï¼ˆå‰3åï¼‰:")
        for i, (alg, rate) in enumerate(rankings['success_ranking'][:3], 1):
            print(f"   {i}. {alg}: {rate:.2%}")

        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    framework = USVTestFramework()

    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å¿«é€Ÿæµ‹è¯•ï¼ˆ5ä¸ªæ¡ˆä¾‹ï¼Œæ‰€æœ‰ç®—æ³•ï¼‰")
    print("2. æ ‡å‡†æµ‹è¯•ï¼ˆ20ä¸ªæ¡ˆä¾‹ï¼Œæ‰€æœ‰ç®—æ³•ï¼‰")
    print("3. è‡ªå®šä¹‰æµ‹è¯•")

    choice = input("è¯·è¾“å…¥é€‰æ‹©ï¼ˆ1-3ï¼‰: ").strip()

    if choice == '1':
        # å¿«é€Ÿæµ‹è¯•
        test_cases = framework.select_test_cases(5)
        df = framework.run_comprehensive_test(test_cases)
        report = framework.generate_analysis_report(df)
        framework.save_results(df, report)
        framework._display_summary(report)

    elif choice == '2':
        # æ ‡å‡†æµ‹è¯•
        framework.run_full_test()

    elif choice == '3':
        # è‡ªå®šä¹‰æµ‹è¯•
        print("\nå¯ç”¨ç®—æ³•:")
        for i, alg in enumerate(framework.algorithms.keys(), 1):
            print(f"{i}. {alg}")

        selected_indices = input("è¯·é€‰æ‹©è¦æµ‹è¯•çš„ç®—æ³•ç¼–å·ï¼ˆç”¨é€—å·åˆ†éš”ï¼Œå¦‚1,3,5ï¼‰: ").strip()
        try:
            indices = [int(x.strip()) for x in selected_indices.split(',')]
            selected_algorithms = [list(framework.algorithms.keys())[i-1] for i in indices]

            num_cases = int(input("è¯·è¾“å…¥æµ‹è¯•æ¡ˆä¾‹æ•°é‡ï¼ˆå»ºè®®5-20ï¼‰: ").strip())
            test_cases = framework.select_test_cases(num_cases)

            df = framework.run_comprehensive_test(test_cases, selected_algorithms)
            report = framework.generate_analysis_report(df)
            framework.save_results(df, report)
            framework._display_summary(report)

        except ValueError:
            print("è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¿è¡Œç¨‹åº")

    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œæ ‡å‡†æµ‹è¯•")
        framework.run_full_test()


if __name__ == "__main__":
    main()